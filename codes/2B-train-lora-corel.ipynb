{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train LoRA for Corel Dataset\n",
        "\n",
        "This notebook trains a LoRA (Low-Rank Adaptation) model for Stable Diffusion using the Corel dataset.\n",
        "\n",
        "**Training options:**\n",
        "- **Unified model**: All classes together (`corel_all`)\n",
        "- **Per-class model**: One class at a time (`class_XXXX`)\n",
        "\n",
        "## Instructions for Google Colab\n",
        "\n",
        "1. Upload your dataset to Google Drive or use the one you already prepared\n",
        "2. Mount Google Drive (see next cell)\n",
        "3. Configure parameters in the \"Configuration\" section\n",
        "4. Run all cells in order\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "**Install all required packages for GPU-accelerated training:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies for GPU training\n",
        "# Core dependencies\n",
        "%pip install -q diffusers accelerate peft transformers datasets tqdm torch torchvision\n",
        "\n",
        "# Optional but recommended for GPU optimization\n",
        "%pip install -q bitsandbytes  # 8-bit optimizer (saves memory)\n",
        "%pip install -q xformers  # Memory-efficient attention (optional)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"✓ DEPENDENCIES INSTALLED\")\n",
        "print(\"=\"*60)\n",
        "print(\"Core packages: diffusers, accelerate, peft, transformers\")\n",
        "print(\"Data: datasets\")\n",
        "print(\"Optimization: bitsandbytes (8-bit), xformers (optional)\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive\n",
        "\n",
        "If your dataset is in Google Drive, uncomment and run this cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the following lines if you need to mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from accelerate import utils\n",
        "from accelerate import Accelerator\n",
        "from diffusers import DDPMScheduler, StableDiffusionPipeline\n",
        "from peft import LoraConfig\n",
        "from peft.utils import get_peft_model_state_dict\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "import math\n",
        "from diffusers.optimization import get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn.functional as F\n",
        "from diffusers.utils import convert_state_dict_to_diffusers\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "formatted_date = datetime.now().strftime(r'%Y%m%d-%H%M%S')\n",
        "print(\"Imports completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configuration\n",
        "\n",
        "**IMPORTANT:** Adjust these parameters according to your needs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== CONFIGURATION =====\n",
        "# EASILY CONFIGURABLE FOR LOCAL OR CLOUD EXECUTION\n",
        "\n",
        "# Option 1: For Google Colab (uncomment and adjust path)\n",
        "# BASE_DIR = Path('/content')\n",
        "# BASE_DIR = Path('/content/drive/MyDrive/your_project_path')\n",
        "\n",
        "# Option 2: For local execution\n",
        "BASE_DIR = Path('.')\n",
        "\n",
        "# Option 3: For other cloud services (adjust as needed)\n",
        "# BASE_DIR = Path('/workspace')\n",
        "# BASE_DIR = Path('/data')\n",
        "\n",
        "print(f\"✓ Base directory: {BASE_DIR.absolute()}\")\n",
        "print(f\"✓ Working directory: {Path.cwd()}\")\n",
        "\n",
        "# Path to dataset (must contain captions.json)\n",
        "# Options:\n",
        "#   - \"training_data/corel/corel_all\"  (all classes)\n",
        "#   - \"training_data/corel/class_0001\"  (specific class)\n",
        "# If using Google Drive: \"/content/drive/MyDrive/path/to/training_data/corel/corel_all\"\n",
        "train_data_dir = str(BASE_DIR / \"training_data\" / \"corel\" / \"corel_all\")\n",
        "\n",
        "# Output directory for LoRA weights\n",
        "output_dir = str(BASE_DIR / \"corel_models\")\n",
        "\n",
        "# Base Stable Diffusion model\n",
        "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# LoRA parameters\n",
        "lora_rank = 4\n",
        "lora_alpha = 4\n",
        "\n",
        "# Training parameters\n",
        "num_train_epochs = 200\n",
        "train_batch_size = 1\n",
        "gradient_accumulation_steps = 4\n",
        "learning_rate = 1e-4\n",
        "resolution = 512\n",
        "\n",
        "# Sample limit (None = use all, useful for quick tests)\n",
        "max_samples = None  # Example: 10 for quick testing\n",
        "\n",
        "# ===== END CONFIGURATION =====\n",
        "\n",
        "# Setup\n",
        "utils.write_basic_config()\n",
        "\n",
        "# Create output directory\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine dataset name from path\n",
        "data_path = Path(train_data_dir)\n",
        "dataset_name = data_path.name  # e.g., \"corel_all\" or \"class_0001\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Data directory: {train_data_dir}\")\n",
        "print(f\"Dataset name: {dataset_name}\")\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "print(f\"Pretrained model: {pretrained_model_name_or_path}\")\n",
        "print(f\"LoRA rank: {lora_rank}, alpha: {lora_alpha}\")\n",
        "print(f\"Epochs: {num_train_epochs}, Batch size: {train_batch_size}\")\n",
        "print(f\"Max samples: {max_samples if max_samples else 'All'}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Initialize Accelerator and Load Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Accelerator\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    mixed_precision=\"fp16\"\n",
        ")\n",
        "device = accelerator.device\n",
        "\n",
        "# Load models\n",
        "print(\"\\nLoading pretrained Stable Diffusion model...\")\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
        ")\n",
        "weight_dtype = torch.float16\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    torch_dtype=weight_dtype\n",
        ").to(device)\n",
        "tokenizer, text_encoder, vae, unet = (\n",
        "    pipe.tokenizer, pipe.text_encoder, pipe.vae, pipe.unet\n",
        ")\n",
        "\n",
        "print(\"Models loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Configure Optimizations and LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable optimizations\n",
        "unet.enable_gradient_checkpointing()\n",
        "text_encoder.gradient_checkpointing_enable()\n",
        "\n",
        "# Try to enable xFormers (optional, saves memory)\n",
        "try:\n",
        "    unet.enable_xformers_memory_efficient_attention()\n",
        "    print(\"xFormers enabled for UNet\")\n",
        "except:\n",
        "    print(\"xFormers not available\")\n",
        "\n",
        "# Freeze base models\n",
        "unet.requires_grad_(False)\n",
        "vae.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)\n",
        "\n",
        "# Move VAE to CPU to save memory\n",
        "vae.to('cpu')\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Configure LoRA for UNet\n",
        "unet_lora_config = LoraConfig(\n",
        "    r=lora_rank,\n",
        "    lora_alpha=lora_alpha,\n",
        "    init_lora_weights=\"gaussian\",\n",
        "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n",
        ")\n",
        "unet.add_adapter(unet_lora_config)\n",
        "\n",
        "# Configure LoRA for Text Encoder\n",
        "text_encoder_lora_rank = 4\n",
        "text_encoder_lora_alpha = 4\n",
        "text_encoder_lora_config = LoraConfig(\n",
        "    r=text_encoder_lora_rank,\n",
        "    lora_alpha=text_encoder_lora_alpha,\n",
        "    init_lora_weights=\"gaussian\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
        ")\n",
        "text_encoder.add_adapter(text_encoder_lora_config)\n",
        "print(\"Text encoder LoRA enabled\")\n",
        "\n",
        "# Convert trainable parameters to fp32\n",
        "for param in unet.parameters():\n",
        "    if param.requires_grad:\n",
        "        param.data = param.to(torch.float32)\n",
        "\n",
        "for param in text_encoder.parameters():\n",
        "    if param.requires_grad:\n",
        "        param.data = param.to(torch.float32)\n",
        "\n",
        "print(\"LoRA configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Load and Preprocess Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "print(f\"\\nLoading dataset from {train_data_dir}...\")\n",
        "dataset = load_dataset(\"imagefolder\", data_dir=train_data_dir)\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# Limit samples if specified\n",
        "if max_samples is not None:\n",
        "    train_data = train_data.select(range(min(max_samples, len(train_data))))\n",
        "    print(f\"  Using {len(train_data)} samples (limited from {len(dataset['train'])})\")\n",
        "else:\n",
        "    print(f\"  Using all {len(train_data)} samples\")\n",
        "\n",
        "# Preprocessing\n",
        "dataset_columns = list(train_data.features.keys())\n",
        "image_column, caption_column = dataset_columns[0], dataset_columns[1]\n",
        "\n",
        "def tokenize_captions(examples, is_train=True):\n",
        "    captions = []\n",
        "    for caption in examples[caption_column]:\n",
        "        if isinstance(caption, str):\n",
        "            captions.append(caption)\n",
        "    inputs = tokenizer(\n",
        "        captions,\n",
        "        max_length=tokenizer.model_max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return inputs.input_ids\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.CenterCrop(resolution),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "def preprocess_train(examples):\n",
        "    images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
        "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
        "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
        "    return examples\n",
        "\n",
        "with accelerator.main_process_first():\n",
        "    train_dataset = train_data.with_transform(preprocess_train)\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=train_batch_size,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"Dataset prepared: {len(train_dataloader)} batches\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Configure Optimizer and Scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure optimization parameters\n",
        "text_encoder_lr = 5e-5\n",
        "adam_beta1, adam_beta2 = 0.9, 0.999\n",
        "adam_weight_decay = 1e-2\n",
        "adam_epsilon = 1e-08\n",
        "lr_scheduler_name = \"constant\"\n",
        "max_grad_norm = 1.0\n",
        "use_8bit_adam = True\n",
        "\n",
        "# Get trainable parameters\n",
        "unet_lora_layers = list(filter(lambda p: p.requires_grad, unet.parameters()))\n",
        "text_encoder_lora_layers = list(filter(lambda p: p.requires_grad, text_encoder.parameters()))\n",
        "params_to_optimize = [\n",
        "    {\"params\": unet_lora_layers, \"lr\": learning_rate},\n",
        "    {\"params\": text_encoder_lora_layers, \"lr\": text_encoder_lr}\n",
        "]\n",
        "\n",
        "# Create optimizer (8-bit if available)\n",
        "if use_8bit_adam:\n",
        "    try:\n",
        "        import bitsandbytes as bnb\n",
        "        optimizer = bnb.optim.AdamW8bit(\n",
        "            params_to_optimize,\n",
        "            betas=(adam_beta1, adam_beta2),\n",
        "            weight_decay=adam_weight_decay,\n",
        "            eps=adam_epsilon\n",
        "        )\n",
        "        print(\"Using 8-bit AdamW\")\n",
        "    except ImportError:\n",
        "        print(\"bitsandbytes not available, using regular AdamW\")\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            params_to_optimize,\n",
        "            betas=(adam_beta1, adam_beta2),\n",
        "            weight_decay=adam_weight_decay,\n",
        "            eps=adam_epsilon\n",
        "        )\n",
        "else:\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        params_to_optimize,\n",
        "        betas=(adam_beta1, adam_beta2),\n",
        "        weight_decay=adam_weight_decay,\n",
        "        eps=adam_epsilon\n",
        "    )\n",
        "\n",
        "lr_scheduler = get_scheduler(lr_scheduler_name, optimizer=optimizer)\n",
        "\n",
        "# Prepare with accelerator\n",
        "unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "max_train_steps = num_train_epochs * len(train_dataloader)\n",
        "progress_bar = tqdm(\n",
        "    range(0, max_train_steps),\n",
        "    initial=0,\n",
        "    desc=\"Steps\",\n",
        "    disable=not accelerator.is_local_main_process,\n",
        ")\n",
        "\n",
        "print(f\"Optimizer configured: {max_train_steps} total steps\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Loop\n",
        "\n",
        "**WARNING: This may take a long time. In Colab, make sure your session doesn't expire.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "import time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total epochs: {num_train_epochs}\")\n",
        "print(f\"Batches per epoch: {len(train_dataloader)}\")\n",
        "print(f\"Total steps: {max_train_steps}\")\n",
        "print(f\"Batch size: {train_batch_size}\")\n",
        "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {train_batch_size * gradient_accumulation_steps}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Text encoder LR: {text_encoder_lr}\")\n",
        "print(f\"LoRA rank: {lora_rank}, alpha: {lora_alpha}\")\n",
        "print(f\"Mixed precision: fp16\")\n",
        "print(f\"Max grad norm: {max_grad_norm}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "best_loss = float('inf')\n",
        "loss_history = []\n",
        "epoch_losses = []\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EPOCH {epoch+1}/{num_train_epochs}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    unet.train()\n",
        "    text_encoder.train()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_steps_in_epoch = 0\n",
        "    train_loss = 0.0\n",
        "    total_grad_norm = 0.0\n",
        "    grad_norm_count = 0\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        with accelerator.accumulate(unet):\n",
        "            # Move VAE to GPU for encoding\n",
        "            vae.to(device)\n",
        "            with torch.no_grad():\n",
        "                latents = vae.encode(\n",
        "                    batch[\"pixel_values\"].to(dtype=weight_dtype)\n",
        "                ).latent_dist.sample()\n",
        "                latents = latents * vae.config.scaling_factor\n",
        "            vae.to('cpu')\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "            # Sample noise\n",
        "            noise = torch.randn_like(latents)\n",
        "            \n",
        "            # Sample timesteps\n",
        "            batch_size = latents.shape[0]\n",
        "            timesteps = torch.randint(\n",
        "                low=0,\n",
        "                high=noise_scheduler.config.num_train_timesteps,\n",
        "                size=(batch_size,),\n",
        "                device=latents.device\n",
        "            ).long()\n",
        "            \n",
        "            # Get text embeddings\n",
        "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "            \n",
        "            # Add noise\n",
        "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "            \n",
        "            # Get target\n",
        "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                target = noise\n",
        "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "            \n",
        "            # Predict\n",
        "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "            \n",
        "            # Accumulate losses\n",
        "            avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()\n",
        "            train_loss += avg_loss.item() / gradient_accumulation_steps\n",
        "            \n",
        "            # Backpropagation\n",
        "            accelerator.backward(loss)\n",
        "            if accelerator.sync_gradients:\n",
        "                params_to_clip = unet_lora_layers + text_encoder_lora_layers\n",
        "                grad_norm = accelerator.clip_grad_norm_(params_to_clip, max_grad_norm)\n",
        "                total_grad_norm += grad_norm.item()\n",
        "                grad_norm_count += 1\n",
        "            \n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                epoch_loss += train_loss\n",
        "                train_loss = 0.0\n",
        "                num_steps_in_epoch += 1\n",
        "            \n",
        "            current_lr = lr_scheduler.get_last_lr()[0]\n",
        "            logs = {\n",
        "                \"epoch\": f\"{epoch+1}/{num_train_epochs}\",\n",
        "                \"loss\": f\"{loss.detach().item():.4f}\",\n",
        "                \"avg\": f\"{avg_loss.item():.4f}\",\n",
        "                \"lr\": f\"{current_lr:.2e}\",\n",
        "                \"step\": f\"{step+1}/{len(train_dataloader)}\"\n",
        "            }\n",
        "            if accelerator.sync_gradients and grad_norm_count > 0:\n",
        "                logs[\"grad\"] = f\"{grad_norm.item():.2f}\"\n",
        "            progress_bar.set_postfix(**logs)\n",
        "        \n",
        "        # Periodic cleanup\n",
        "        if step % 10 == 0 and step > 0:\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    # Calculate average epoch loss\n",
        "    if num_steps_in_epoch > 0:\n",
        "        avg_epoch_loss = epoch_loss / num_steps_in_epoch\n",
        "        avg_grad_norm = total_grad_norm / grad_norm_count if grad_norm_count > 0 else 0.0\n",
        "    else:\n",
        "        avg_epoch_loss = epoch_loss\n",
        "        avg_grad_norm = 0.0\n",
        "    \n",
        "    loss_history.append(avg_epoch_loss)\n",
        "    epoch_losses.append(avg_epoch_loss)\n",
        "    \n",
        "    # Calculate epoch time\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    \n",
        "    # Check if this is a new best\n",
        "    is_new_best = avg_epoch_loss < best_loss\n",
        "    if is_new_best:\n",
        "        improvement = best_loss - avg_epoch_loss if epoch > 0 else 0\n",
        "        best_loss = avg_epoch_loss\n",
        "    \n",
        "    # Print epoch summary\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"  Average Loss: {avg_epoch_loss:.6f}\")\n",
        "    print(f\"  Steps in epoch: {num_steps_in_epoch}\")\n",
        "    print(f\"  Learning Rate: {current_lr:.2e}\")\n",
        "    if avg_grad_norm > 0:\n",
        "        print(f\"  Average Grad Norm: {avg_grad_norm:.4f}\")\n",
        "    print(f\"  Best Loss So Far: {best_loss:.6f}\")\n",
        "    print(f\"  Epoch Time: {epoch_time:.1f}s ({epoch_time/60:.1f} min)\")\n",
        "    \n",
        "    if is_new_best:\n",
        "        if epoch > 0:\n",
        "            print(f\"  Status: NEW BEST MODEL! Improved by {improvement:.6f}\")\n",
        "        else:\n",
        "            print(f\"  Status: First epoch (baseline)\")\n",
        "    else:\n",
        "        diff = avg_epoch_loss - best_loss\n",
        "        print(f\"  Status: No improvement (+{diff:.6f} from best)\")\n",
        "    \n",
        "    # GPU memory info\n",
        "    if torch.cuda.is_available() and accelerator.is_main_process:\n",
        "        memory_allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
        "        memory_reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
        "        print(f\"  GPU Memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved\")\n",
        "    \n",
        "    # Progress percentage and time estimates\n",
        "    progress_pct = ((epoch + 1) / num_train_epochs) * 100\n",
        "    elapsed_time = time.time() - training_start_time\n",
        "    avg_time_per_epoch = elapsed_time / (epoch + 1)\n",
        "    remaining_epochs = num_train_epochs - (epoch + 1)\n",
        "    estimated_remaining = avg_time_per_epoch * remaining_epochs\n",
        "    \n",
        "    print(f\"  Progress: {progress_pct:.1f}% ({epoch+1}/{num_train_epochs} epochs)\")\n",
        "    print(f\"  Elapsed: {elapsed_time/60:.1f} min | Estimated remaining: {estimated_remaining/60:.1f} min\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "total_training_time = time.time() - training_start_time\n",
        "print(f\"Total epochs completed: {len(epoch_losses)}\")\n",
        "print(f\"Total training time: {total_training_time/60:.1f} min ({total_training_time/3600:.2f} hours)\")\n",
        "print(f\"Average time per epoch: {total_training_time/len(epoch_losses)/60:.1f} min\")\n",
        "print(f\"Best loss achieved: {best_loss:.6f}\")\n",
        "print(f\"Final loss: {epoch_losses[-1]:.6f}\")\n",
        "print(f\"Total steps: {max_train_steps}\")\n",
        "print(f\"Loss improvement: {epoch_losses[0] - best_loss:.6f} (from {epoch_losses[0]:.6f} to {best_loss:.6f})\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save LoRA Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA weights\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SAVING LoRA WEIGHTS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "if accelerator.is_main_process:\n",
        "    print(\"Preparing models for saving...\")\n",
        "    unet = unet.to(torch.float32)\n",
        "    text_encoder = text_encoder.to(torch.float32)\n",
        "    print(\"  Models converted to float32\")\n",
        "    \n",
        "    print(\"Extracting LoRA weights...\")\n",
        "    unwrapped_unet = accelerator.unwrap_model(unet)\n",
        "    unet_lora_state_dict = convert_state_dict_to_diffusers(\n",
        "        get_peft_model_state_dict(unwrapped_unet)\n",
        "    )\n",
        "    print(f\"  UNet LoRA weights extracted: {len(unet_lora_state_dict)} parameters\")\n",
        "    \n",
        "    unwrapped_text_encoder = accelerator.unwrap_model(text_encoder)\n",
        "    text_encoder_lora_state_dict = convert_state_dict_to_diffusers(\n",
        "        get_peft_model_state_dict(unwrapped_text_encoder)\n",
        "    )\n",
        "    print(f\"  Text Encoder LoRA weights extracted: {len(text_encoder_lora_state_dict)} parameters\")\n",
        "    \n",
        "    weight_name = (\n",
        "        f\"lora_{pretrained_model_name_or_path.split('/')[-1]}_\"\n",
        "        f\"rank{lora_rank}_s{max_train_steps}_r{resolution}_\"\n",
        "        f\"DDPMScheduler_{dataset_name}_{formatted_date}.safetensors\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nSaving LoRA weights to file...\")\n",
        "    print(f\"  Filename: {weight_name}\")\n",
        "    print(f\"  Directory: {output_dir}\")\n",
        "    \n",
        "    StableDiffusionPipeline.save_lora_weights(\n",
        "        save_directory=output_dir,\n",
        "        unet_lora_layers=unet_lora_state_dict,\n",
        "        text_encoder_lora_layers=text_encoder_lora_state_dict,\n",
        "        safe_serialization=True,\n",
        "        weight_name=weight_name\n",
        "    )\n",
        "    \n",
        "    # Get file size\n",
        "    import os\n",
        "    file_path = os.path.join(output_dir, weight_name)\n",
        "    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"LoRA SAVED SUCCESSFULLY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"File: {weight_name}\")\n",
        "    print(f\"Location: {output_dir}\")\n",
        "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
        "    print(f\"Includes: UNet LoRA + Text Encoder LoRA\")\n",
        "    print(f\"LoRA rank: {lora_rank}, alpha: {lora_alpha}\")\n",
        "    print(f\"Training steps: {max_train_steps}\")\n",
        "    print(f\"Best loss: {best_loss:.6f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Show how to download the file in Colab\n",
        "    print(f\"\\nTo download in Colab, run:\")\n",
        "    print(f\"from google.colab import files\")\n",
        "    print(f\"files.download('{file_path}')\")\n",
        "    \n",
        "    # Optionally save to Google Drive\n",
        "    print(f\"\\nTo save to Google Drive, run:\")\n",
        "    print(f\"import shutil\")\n",
        "    print(f\"drive_path = '/content/drive/MyDrive/corel_models/{weight_name}'\")\n",
        "    print(f\"shutil.copy('{file_path}', drive_path)\")\n",
        "    print(f\"print(f'Copied to Google Drive: {{drive_path}}')\")\n",
        "else:\n",
        "    print(\"Waiting for main process to save weights...\")\n",
        "\n",
        "accelerator.end_training()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
