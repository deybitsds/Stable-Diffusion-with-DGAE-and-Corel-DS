{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Images with Corel LoRA\n",
        "\n",
        "This notebook generates images using LoRA weights trained on the Corel dataset.\n",
        "\n",
        "## Instructions for Google Colab\n",
        "\n",
        "1. Upload trained LoRA weights (`.safetensors` file) to Colab or Google Drive\n",
        "2. Configure parameters in the \"Configuration\" section\n",
        "3. Run all cells in order\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "**Install all required packages for GPU-accelerated generation:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies for GPU generation\n",
        "%pip install -q diffusers accelerate transformers torch torchvision\n",
        "\n",
        "# Optional but recommended for GPU optimization\n",
        "%pip install -q xformers  # Memory-efficient attention (optional)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"✓ DEPENDENCIES INSTALLED\")\n",
        "print(\"=\"*60)\n",
        "print(\"Core packages: diffusers, accelerate, transformers\")\n",
        "print(\"Optimization: xformers (optional, saves memory)\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive\n",
        "\n",
        "If your LoRA weights are in Google Drive, uncomment and run this cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the following lines if you need to mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "from diffusers.utils import make_image_grid\n",
        "from diffusers import EulerDiscreteScheduler\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "print(\"Imports completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configuration\n",
        "\n",
        "**IMPORTANT:** Adjust these parameters according to your needs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== CONFIGURATION =====\n",
        "# EASILY CONFIGURABLE FOR LOCAL OR CLOUD EXECUTION\n",
        "\n",
        "# Option 1: For Google Colab (uncomment and adjust path)\n",
        "# BASE_DIR = Path('/content')\n",
        "# BASE_DIR = Path('/content/drive/MyDrive/your_project_path')\n",
        "\n",
        "# Option 2: For local execution\n",
        "BASE_DIR = Path('.')\n",
        "\n",
        "# Option 3: For other cloud services (adjust as needed)\n",
        "# BASE_DIR = Path('/workspace')\n",
        "# BASE_DIR = Path('/data')\n",
        "\n",
        "print(f\"✓ Base directory: {BASE_DIR.absolute()}\")\n",
        "print(f\"✓ Working directory: {Path.cwd()}\")\n",
        "\n",
        "# Directory with LoRA weights\n",
        "# If using Google Drive: \"/content/drive/MyDrive/path/to/corel_models\"\n",
        "lora_dir = str(BASE_DIR / \"corel_models\")\n",
        "\n",
        "# Specific LoRA file name (None = use the most recent)\n",
        "lora_name = None  # Example: \"lora_stable-diffusion-v1-5_rank4_s800_r512_DDPMScheduler_corel_all_20250101-120000.safetensors\"\n",
        "\n",
        "# Base Stable Diffusion model\n",
        "pretrained_model = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# Text prompt (None = use automatic prompts based on LoRA name)\n",
        "prompt = None  # Example: \"a photo of a royalguard\"\n",
        "\n",
        "# Generation parameters\n",
        "num_images = 4\n",
        "width = 512\n",
        "height = 512\n",
        "guidance_scale = 7.5\n",
        "seed = 42\n",
        "\n",
        "# Output directory\n",
        "output_dir = str(BASE_DIR / \"generated_images\")\n",
        "\n",
        "# ===== END CONFIGURATION =====\n",
        "\n",
        "# Create output directory\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Verify CUDA availability and GPU info\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"✓ CUDA AVAILABLE - GPU OPTIMIZATION ENABLED\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"PyTorch Version: {torch.__version__}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Set CUDA optimizations\n",
        "    torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes\n",
        "    torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed\n",
        "    print(\"✓ CUDA optimizations enabled (cudnn.benchmark=True)\\n\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"\\n⚠ WARNING: CUDA not available! Generation will be VERY slow on CPU.\")\n",
        "    print(\"⚠ This notebook is optimized for GPU. Consider using a GPU-enabled environment.\")\n",
        "    print(f\"⚠ Device: {device}\\n\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GENERATION CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"LoRA directory: {lora_dir}\")\n",
        "print(f\"LoRA name: {lora_name if lora_name else 'Latest'}\")\n",
        "print(f\"Base model: {pretrained_model}\")\n",
        "print(f\"Prompt: {prompt if prompt else 'Auto (from LoRA name)'}\")\n",
        "print(f\"Number of images: {num_images}\")\n",
        "print(f\"Resolution: {width}x{height}\")\n",
        "print(f\"Guidance scale: {guidance_scale}\")\n",
        "print(f\"Seed: {seed}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Find LoRA File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find LoRA file\n",
        "lora_path = Path(lora_dir)\n",
        "\n",
        "if lora_name:\n",
        "    lora_file = lora_path / lora_name\n",
        "    if not lora_file.exists():\n",
        "        raise FileNotFoundError(f\"LoRA file not found: {lora_file}\")\n",
        "    print(f\"Using specific LoRA: {lora_name}\")\n",
        "else:\n",
        "    # Find the most recent file\n",
        "    lora_files = list(lora_path.glob(\"*.safetensors\"))\n",
        "    if not lora_files:\n",
        "        raise FileNotFoundError(f\"No LoRA files found in {lora_dir}\")\n",
        "    lora_file = sorted(lora_files, key=lambda x: x.stat().st_mtime)[-1]\n",
        "    lora_name = lora_file.name\n",
        "    print(f\"Using most recent LoRA: {lora_name}\")\n",
        "\n",
        "print(f\"Full file path: {lora_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Model and LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Stable Diffusion model\n",
        "print(\"\\nLoading Stable Diffusion model...\")\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    pretrained_model,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None  # Remove safety checker to save VRAM\n",
        ").to(device)\n",
        "\n",
        "# Enable memory optimizations\n",
        "pipe.enable_attention_slicing(slice_size=1)\n",
        "pipe.enable_vae_tiling()\n",
        "print(\"Memory optimizations enabled\")\n",
        "\n",
        "# Load LoRA weights\n",
        "print(f\"\\nLoading LoRA: {lora_name}\")\n",
        "pipe.load_lora_weights(\n",
        "    pretrained_model_name_or_path_or_dict=str(lora_path),\n",
        "    weight_name=lora_name,\n",
        "    adapter_name=\"corel_lora\"\n",
        ")\n",
        "pipe.set_adapters([\"corel_lora\"], adapter_weights=[1.0])\n",
        "print(\"LoRA loaded successfully\")\n",
        "\n",
        "# Configure scheduler\n",
        "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "print(\"Scheduler configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Determine Prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine prompts based on LoRA name or use provided prompt\n",
        "if prompt:\n",
        "    prompts = [prompt]\n",
        "else:\n",
        "    # Extract information from LoRA filename\n",
        "    if \"class_\" in lora_name:\n",
        "        # Try to extract class number\n",
        "        match = re.search(r'class_(\\d+)', lora_name)\n",
        "        if match:\n",
        "            class_num = match.group(1)\n",
        "            prompts = [f\"a photo of a corel class {class_num}\"]\n",
        "        else:\n",
        "            prompts = [\"a photo of a corel image\"]\n",
        "    elif \"corel_all\" in lora_name:\n",
        "        # Generic prompts for all-classes model\n",
        "        prompts = [\n",
        "            \"a photo of a corel image\",\n",
        "            \"a high quality corel image\",\n",
        "            \"a detailed corel photograph\"\n",
        "        ]\n",
        "    else:\n",
        "        prompts = [\"a photo of a corel image\"]\n",
        "\n",
        "print(f\"Prompts determined: {len(prompts)}\")\n",
        "for i, p in enumerate(prompts):\n",
        "    print(f\"  {i+1}. {p}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Generate Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate images\n",
        "all_images = []\n",
        "all_prompts = []\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "negative_prompt = \"low quality, blur, watermark, words, name, text\"\n",
        "\n",
        "for prompt_idx, current_prompt in enumerate(prompts):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"GENERATING - Prompt {prompt_idx+1}/{len(prompts)}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Prompt: {current_prompt}\")\n",
        "    print(f\"Resolution: {width}x{height}\")\n",
        "    print(f\"Number of images: {num_images}\")\n",
        "    print(f\"Guidance scale: {guidance_scale}\")\n",
        "    print(f\"Seed: {seed}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Generate images one at a time for better memory management\n",
        "    images = []\n",
        "    \n",
        "    for i in range(num_images):\n",
        "        print(f\"\\n[{i+1}/{num_images}] Generating image...\")\n",
        "        \n",
        "        # Create generator on the correct device\n",
        "        if device.startswith(\"cuda\"):\n",
        "            generator = torch.Generator(device=device)\n",
        "        else:\n",
        "            generator = torch.Generator()\n",
        "        generator.manual_seed(seed + i)\n",
        "        \n",
        "        image = pipe(\n",
        "            prompt=current_prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_images_per_prompt=1,\n",
        "            generator=generator,\n",
        "            width=width,\n",
        "            height=height,\n",
        "            guidance_scale=guidance_scale\n",
        "        ).images[0]\n",
        "        \n",
        "        images.append(image)\n",
        "        \n",
        "        # Save individual image\n",
        "        safe_prompt = \"\".join(c for c in current_prompt[:30] if c.isalnum() or c in (' ', '-', '_')).strip()\n",
        "        safe_prompt = safe_prompt.replace(' ', '_')\n",
        "        img_path = Path(output_dir) / f\"corel_{safe_prompt}_{timestamp}_{i+1:02d}.png\"\n",
        "        image.save(img_path)\n",
        "        print(f\"  Saved: {img_path.name}\")\n",
        "        \n",
        "        # Clear cache between generations\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    all_images.extend(images)\n",
        "    all_prompts.extend([current_prompt] * len(images))\n",
        "    \n",
        "    # Create grid for this prompt\n",
        "    if len(images) > 1:\n",
        "        grid = make_image_grid(images, cols=min(4, len(images)), rows=1)\n",
        "        grid_path = Path(output_dir) / f\"corel_grid_{safe_prompt}_{timestamp}.png\"\n",
        "        grid.save(grid_path)\n",
        "        print(f\"  Grid saved: {grid_path.name}\")\n",
        "\n",
        "print(\"\\nGeneration completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Create Overall Grid and Visualize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create overall grid if multiple prompts\n",
        "if len(all_images) > 1:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Creating overall grid...\")\n",
        "    overall_grid = make_image_grid(all_images, cols=min(4, len(all_images)), rows=len(prompts))\n",
        "    overall_grid_path = Path(output_dir) / f\"corel_all_grid_{timestamp}.png\"\n",
        "    overall_grid.save(overall_grid_path)\n",
        "    print(f\"Overall grid saved: {overall_grid_path.name}\")\n",
        "\n",
        "# Display images in notebook\n",
        "from IPython.display import display, Image as IPImage\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATED IMAGES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if len(all_images) > 1:\n",
        "    display(overall_grid)\n",
        "else:\n",
        "    display(all_images[0])\n",
        "\n",
        "print(f\"\\nTotal images generated: {len(all_images)}\")\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "print(\"\\nGenerated files:\")\n",
        "for img_path in sorted(Path(output_dir).glob(f\"corel_*_{timestamp}*.png\")):\n",
        "    print(f\"  - {img_path.name}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Cleanup\n",
        "pipe.to(\"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\nCleanup completed\")\n",
        "\n",
        "# Optionally save to Google Drive\n",
        "print(\"\\nTo save to Google Drive, run:\")\n",
        "print(\"import shutil\")\n",
        "print(f\"for img_path in Path('{output_dir}').glob('corel_*_{timestamp}*.png'):\")\n",
        "print(\"    drive_path = f'/content/drive/MyDrive/generated_images/{img_path.name}'\")\n",
        "print(\"    shutil.copy(img_path, drive_path)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
